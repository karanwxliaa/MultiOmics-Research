{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy\n",
    "import pandas\n",
    "import captum\n",
    "import tqdm\n",
    "import scipy\n",
    "import scanpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn - Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "global mu\n",
    "global var\n",
    "\n",
    "class LinBnDrop(nn.Sequential):\n",
    "    \"\"\"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers\"\"\"\n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=True):\n",
    "        layers = [nn.BatchNorm1d(n_out if lin_first else n_in)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        if act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)\n",
    "\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder for CITE-seq data\"\"\"\n",
    "    def __init__(self, nfeatures_modality1=10703, nfeatures_modality2=192, hidden_modality1=185,  hidden_modality2=15, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.nfeatures_modality1 = nfeatures_modality1\n",
    "        self.nfeatures_modality2 = nfeatures_modality2\n",
    "        self.encoder_modality1 = LinBnDrop(nfeatures_modality1, hidden_modality1, p=0.2, act=nn.ReLU())\n",
    "        self.encoder_modality2 = LinBnDrop(nfeatures_modality2, hidden_modality2, p=0.2, act=nn.ReLU())\n",
    "        self.encoder = LinBnDrop(hidden_modality1 + hidden_modality2, z_dim,  p=0.2, act=nn.ReLU())\n",
    "        self.weights_modality1 = nn.Parameter(torch.rand((1,nfeatures_modality1)) * 0.001, requires_grad=True)\n",
    "        self.weights_modality2 = nn.Parameter(torch.rand((1,nfeatures_modality2)) * 0.001, requires_grad=True)\n",
    "        self.fc_mu = LinBnDrop(z_dim,z_dim, p=0.2)\n",
    "        self.fc_var = LinBnDrop(z_dim,z_dim, p=0.2)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        global mu\n",
    "        global var\n",
    "        x_modality1 = self.encoder_modality1(x[:, :self.nfeatures_modality1]*self.weights_modality1)\n",
    "        x_modality2 = self.encoder_modality2(x[:, self.nfeatures_modality1:]*self.weights_modality2)\n",
    "        x = torch.cat([x_modality1, x_modality2], 1)\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        var = self.fc_var(x)\n",
    "        x = self.reparameterize(mu, var)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder for for 2 modalities data (citeseq data and shareseq data) \"\"\"\n",
    "    def __init__(self, nfeatures_modality1=10703, nfeatures_modality2=192,  hidden_modality1=185,  hidden_modality2=15, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.nfeatures_modality1 = nfeatures_modality1\n",
    "        self.nfeatures_modality2 = nfeatures_modality2\n",
    "        self.decoder1 = LinBnDrop(z_dim, nfeatures_modality1, act=nn.ReLU())\n",
    "        self.decoder2 = LinBnDrop(z_dim, nfeatures_modality2,  act=nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_rna = self.decoder1(x)\n",
    "        x_adt = self.decoder2(x)\n",
    "        x = torch.cat((x_rna,x_adt),1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class CiteAutoencoder_CITEseq(nn.Module):\n",
    "    def __init__(self, nfeatures_rna=0, nfeatures_adt=0,  hidden_rna=185,  hidden_adt=15, z_dim=20,classify_dim=17):\n",
    "        \"\"\" Autoencoder for 2 modalities data (citeseq data and shareseq data) \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(nfeatures_rna, nfeatures_adt, hidden_rna,  hidden_adt, z_dim)\n",
    "        self.classify = nn.Linear(z_dim, classify_dim)\n",
    "        self.decoder = Decoder(nfeatures_rna, nfeatures_adt, hidden_rna,  hidden_adt, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        global mu\n",
    "        global var\n",
    "        x = self.encoder(x)\n",
    "        x_cty = self.classify(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, x_cty,mu,var\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn - Train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from util import AverageMeter,accuracy,save_checkpoint,CrossEntropyLabelSmooth,KL_loss\n",
    "\n",
    "def train_model(model, train_dl, test_dl, lr, epochs, classify_dim=17, best_top1_acc=0, save_path = \"\", feature_num=10000):\n",
    "    #####set optimizer and criterin#####\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) ##\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    criterion_smooth_cty = CrossEntropyLabelSmooth().to(device)\n",
    "    \n",
    "    best_top1_acc=0\n",
    "    best_each_celltype_top1 = []\n",
    "    best_each_celltype_num=[]\n",
    "    train_each_celltype_num = []\n",
    "\n",
    "    for i in range(classify_dim):\n",
    "        best_each_celltype_top1.append(0)\n",
    "        best_each_celltype_num.append(0)\n",
    "        train_each_celltype_num.append(0)\n",
    "                \n",
    "    ######loop training process, each epoch contains train and test two part#########\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.train()\n",
    "        nsamples_train = 0\n",
    "        train_top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "        model = model.train()\n",
    "        nsamples_test = 0\n",
    "        test_top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "        each_celltype_top1 = []\n",
    "        each_celltype_num=[]\n",
    "        for i in range(classify_dim):\n",
    "            each_celltype_top1.append(AverageMeter('Acc@1', ':6.2f'))\n",
    "            each_celltype_num.append(0)\n",
    "            \n",
    "\n",
    "            \n",
    "        for i, batch_sample in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            ###load data\n",
    "            x = batch_sample['data']\n",
    "            x = Variable(x)\n",
    "            x = torch.reshape(x,(x.size(0),-1))\n",
    "            train_label = batch_sample['label']\n",
    "            train_label = Variable(train_label)\n",
    "            # Forward pass\n",
    "            x_prime, x_cty,mu, var = model(x.to(device))\n",
    "            # loss function\n",
    "            loss1 = criterion(x_prime, x.to(device)) + 1/feature_num*(KL_loss(mu,var))#simulation loss\n",
    "            loss2 = criterion_smooth_cty(x_cty, train_label.to(device))  #classification loss\n",
    "            loss = 0.9*loss1 + 0.1*loss2 ##sum up the loss together\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # log losses\n",
    "            batch_size = x.shape[0]\n",
    "            nsamples_train += batch_size\n",
    "            train_pred1,  = accuracy(x_cty, train_label, topk=(1, ))\n",
    "            train_top1.update(train_pred1[0], 1)\n",
    "            if epoch == 1:\n",
    "                for j in range(classify_dim):\n",
    "                    if len(train_label[train_label==j])!=0:\n",
    "                        train_each_celltype_num[j]=train_each_celltype_num[j] + len(train_label[train_label==j])                        \n",
    "\n",
    "\n",
    "        model = model.eval()\n",
    "        if test_dl!=\"NULL\":\n",
    "            with torch.no_grad():\n",
    "                for i, batch_sample in enumerate(test_dl):\n",
    "                    ###load data\n",
    "                    x = batch_sample['data']\n",
    "                    x = Variable(x)\n",
    "                    x = torch.reshape(x,(x.size(0),-1))\n",
    "                    test_label = batch_sample['label']    \n",
    "                    test_label = Variable(test_label)\n",
    "                    ###forward process\n",
    "                    x_prime, x_cty,mu, var = model(x.to(device))\n",
    "\n",
    "                    batch_size = x.shape[0]\n",
    "                    nsamples_test += batch_size\n",
    "                    test_pred1,  = accuracy(x_cty, test_label, topk=(1, ))\n",
    "                    test_top1.update(test_pred1[0], 1)\n",
    "                \n",
    "                    ###record accuracy for each celltype\n",
    "                    for j in range(classify_dim):\n",
    "                        if len(test_label[test_label==j])!=0:\n",
    "                            pred1,  = accuracy(x_cty[test_label==j,:], test_label[test_label==j], topk=(1, ))\n",
    "                            each_celltype_top1[j].update(pred1[0],1)\n",
    "                            each_celltype_num[j]=each_celltype_num[j] + len(test_label[test_label==j])\n",
    "        \n",
    "            ####save the best model\n",
    "        #if test_top1.avg > best_top1_acc:\n",
    "        #    best_top1_acc = test_top1.avg\n",
    "        if epoch==epochs:\n",
    "            #for j in range(classify_dim):\n",
    "            #    best_each_celltype_top1[j] = each_celltype_top1[j].avg\n",
    "            #    best_each_celltype_num[j] = each_celltype_num[j]\n",
    "            save_checkpoint({'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                #'best_top1_acc': best_top1_acc,\n",
    "                #'best_top1_celltype_acc': best_each_celltype_top1,\n",
    "                #'best_top1_celltype_num': best_each_celltype_num,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                }, save_path)\n",
    "\n",
    "        \n",
    "        #if epoch==epochs:\n",
    "        #    print('Epoch : ',epoch, '\\t')\n",
    "        #    for j in range(classify_dim):\n",
    "        #        print('cell type : ',j, '\\t', '\\t', 'prec :', best_each_celltype_top1[j], 'number:', best_each_celltype_num[j], 'train_cty_num:',train_each_celltype_num[j])\n",
    "    \n",
    "    return model,best_each_celltype_top1,best_each_celltype_num,train_each_celltype_num\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn - Predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from util import AverageMeter,accuracy\n",
    "\n",
    "def test_model(model, dl, real_label, classify_dim=17, save_path = \"\"):\n",
    "    #####set optimizer and criterin#####\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nsamples_test = 0\n",
    "    test_top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    each_celltype_top1 = []\n",
    "    each_celltype_num=[]\n",
    "    best_each_celltype_top1 = []\n",
    "    for i in range(classify_dim):\n",
    "        each_celltype_top1.append(AverageMeter('Acc@1', ':6.2f'))\n",
    "        each_celltype_num.append(0)\n",
    "        best_each_celltype_top1.append(0)\n",
    "\n",
    "    model = model.eval()\n",
    "    classified_label = []\n",
    "    groundtruth_label = []\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch_sample in enumerate(dl):\n",
    "            ###load data\n",
    "            x = batch_sample['data']\n",
    "            x = Variable(x)\n",
    "            x = torch.reshape(x,(x.size(0),-1))\n",
    "            test_label = batch_sample['label']    \n",
    "            test_label = Variable(test_label)\n",
    "            ###forward process\n",
    "            x_prime, x_cty,mu, var = model(x.to(device))\n",
    "            a = torch.max(nn.Softmax()(x_cty),1)\n",
    "            \n",
    "            for j in range(x_prime.size(0)):\n",
    "                classified_label.append(real_label[a.indices[j]])\n",
    "                groundtruth_label.append(real_label[test_label[j]])\n",
    "                prob.append(a.values[j])\n",
    "               \n",
    "\n",
    "            batch_size = x.shape[0]\n",
    "            nsamples_test += batch_size\n",
    "            test_pred1,  = accuracy(x_cty, test_label, topk=(1, ))\n",
    "            test_top1.update(test_pred1[0], 1)\n",
    "\n",
    "            ###record accuracy for each celltype\n",
    "            for j in range(classify_dim):\n",
    "                if len(test_label[test_label==j])!=0:\n",
    "                    pred1,  = accuracy(x_cty[test_label==j,:], test_label[test_label==j], topk=(1, ))\n",
    "                    each_celltype_top1[j].update(pred1[0],1)\n",
    "                    each_celltype_num[j]=each_celltype_num[j] + len(test_label[test_label==j])\n",
    "\n",
    "    for j in range(classify_dim):\n",
    "        best_each_celltype_top1[j] = each_celltype_top1[j].avg\n",
    "        print('cell type ID: ',j, '\\t', '\\t', 'cell type:', real_label[j], '\\t', '\\t', 'prec :', each_celltype_top1[j].avg, 'number:', each_celltype_num[j], file = save_path)\n",
    "        \n",
    "    return model,best_each_celltype_top1,each_celltype_num, classified_label, groundtruth_label,prob\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main_matilda_train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KARAN\\AppData\\Local\\Temp\\ipykernel_6384\\3470421011.py:2: DeprecationWarning: The parser module is deprecated and will be removed in future versions of Python\n",
      "  import parser\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import parser\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from captum.attr import *\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from learn.train import train_model\n",
    "from util import setup_seed, MyDataset,ToTensor, read_h5_data, read_fs_label, get_vae_simulated_data_from_sampling, get_encodings, compute_zscore, compute_log2,save_checkpoint\n",
    "\n",
    "\n",
    "def main_matilda_train():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\"Matilda\")\n",
    "    parser.add_argument('--seed', type=int, default=1, help='seed')\n",
    "    parser.add_argument('--augmentation', type=bool, default= True, help='if augmentation or not')\n",
    "\n",
    "    ############# for data build ##############\n",
    "    parser.add_argument('--rna', metavar='DIR', default='NULL', help='path to train rna data')\n",
    "    parser.add_argument('--adt', metavar='DIR', default='NULL', help='path to train adt data')\n",
    "    parser.add_argument('--atac', metavar='DIR', default='NULL', help='path to train atac data')\n",
    "    parser.add_argument('--cty', metavar='DIR', default='NULL', help='path to train cell type label')\n",
    "\n",
    "    ##############  for training #################\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=30, help='num of training epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.02, help='init learning rate')\n",
    "\n",
    "    ############# for model build ##############\n",
    "    parser.add_argument('--z_dim', type=int, default=100, help='the number of neurons in latent space')\n",
    "    parser.add_argument('--hidden_rna', type=int, default=185, help='the number of neurons for RNA layer')\n",
    "    parser.add_argument('--hidden_adt', type=int, default=30, help='the number of neurons for ADT layer')\n",
    "    parser.add_argument('--hidden_atac', type=int, default=185, help='the number of neurons for ATAC layer')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    setup_seed(args.seed) ### set random seed in order to reproduce the result\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "    if args.adt != \"NULL\" and args.atac == \"NULL\":\n",
    "        mode = \"CITEseq\"\n",
    "        train_rna_data_path = args.rna\n",
    "        train_adt_data_path = args.adt\n",
    "        train_label_path = args.cty\n",
    "        train_rna_data = read_h5_data(train_rna_data_path)\n",
    "        train_adt_data = read_h5_data(train_adt_data_path)\n",
    "        train_label = read_fs_label(train_label_path)\n",
    "        classify_dim = (max(train_label)+1).cpu().numpy()\n",
    "        nfeatures_rna = train_rna_data.shape[1]\n",
    "        nfeatures_adt = train_adt_data.shape[1]\n",
    "        feature_num = nfeatures_rna + nfeatures_adt\n",
    "        train_rna_data = compute_log2(train_rna_data)\n",
    "        train_adt_data = compute_log2(train_adt_data)\n",
    "        train_rna_data = compute_zscore(train_rna_data)\n",
    "        train_adt_data = compute_zscore(train_adt_data)\n",
    "        train_data = torch.cat((train_rna_data,train_adt_data),1)\n",
    "        train_transformed_dataset = MyDataset(train_data, train_label)\n",
    "        train_dl = DataLoader(train_transformed_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
    "        \n",
    "        \n",
    "    test_dl = \"NULL\"\n",
    "\n",
    "            \n",
    "    print(\"The dataset is\", mode)    \n",
    "    output_v = []\n",
    "    model_save_path = \"../trained_model/{}/\".format(mode)   \n",
    "    model_save_path_1stage = \"../trained_model/{}/simulation_\".format(mode)    \n",
    "    save_fs_eachcell = \"../output/marker/{}/\".format(mode)   \n",
    "\n",
    "    #######build model#########\n",
    "    if mode == \"CITEseq\":\n",
    "        model = CiteAutoencoder_CITEseq(nfeatures_rna, nfeatures_adt, args.hidden_rna, args.hidden_adt, args.z_dim, classify_dim)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "    #model = nn.DataParallel(model).to(device) #multi gpu\n",
    "    model = model.to(device) #one gpu\n",
    "    ########train model#########\n",
    "    model, acc1, num1, train_num = train_model(model, train_dl, test_dl, lr=args.lr, epochs=args.epochs, classify_dim = classify_dim, best_top1_acc=0, save_path=model_save_path,feature_num=feature_num)\n",
    "    ##################prepare to do augmentation##################            \n",
    "    if args.augmentation == True:\n",
    "        stage1_list = []\n",
    "        for i in np.arange(0, classify_dim):\n",
    "            stage1_list.append([i, train_num[i]])\n",
    "            stage1_df = pd.DataFrame(stage1_list)\n",
    "        if classify_dim%2==0:\n",
    "            train_median = np.sort(train_num)[int(classify_dim/2)-1]\n",
    "        else: \n",
    "            train_median = np.median(train_num)\n",
    "        median_anchor = stage1_df[stage1_df[1] == train_median][0]\n",
    "        train_major = stage1_df[stage1_df[1] > train_median]\n",
    "        train_minor = stage1_df[stage1_df[1] < train_median]\n",
    "        anchor_fold = np.array((train_median)/(train_minor[:][1]))\n",
    "        minor_anchor_cts = train_minor[0].to_numpy()\n",
    "        major_anchor_cts = train_major[0].to_numpy()\n",
    "\n",
    "        index = (train_label == int(np.array(median_anchor))).nonzero(as_tuple=True)[0]\n",
    "        anchor_data = train_data[index.tolist(),:]\n",
    "        anchor_label = train_label[index.tolist()]\n",
    "        new_data = anchor_data \n",
    "        new_label = anchor_label\n",
    "\n",
    "        ##############random downsample major cell types##############\n",
    "        j=0\n",
    "        for anchor in major_anchor_cts:     \n",
    "            anchor_num = np.array(train_major[1])[j]\n",
    "            N = range(anchor_num)\n",
    "            ds_index = random.sample(N,int(train_median))\n",
    "            index = (train_label == anchor).nonzero(as_tuple=True)[0]\n",
    "            anchor_data = train_data[index.tolist(),:]\n",
    "            anchor_label = train_label[index.tolist()]\n",
    "            anchor_data = anchor_data[ds_index,:]\n",
    "            anchor_label = anchor_label[ds_index]\n",
    "            new_data = torch.cat((new_data,anchor_data),0)\n",
    "            new_label = torch.cat((new_label,anchor_label.to(device)),0)\n",
    "            j = j+1\n",
    "\n",
    "        ###############augment for minor cell types##################\n",
    "        j = 0\n",
    "        for anchor in minor_anchor_cts:\n",
    "            aug_fold = int((anchor_fold[j]))    \n",
    "            remaining_cell = int(train_median - (int(anchor_fold[j]))*np.array(train_minor[1])[j])\n",
    "            index = (train_label == anchor).nonzero(as_tuple=True)[0]\n",
    "            anchor_data = train_data[index.tolist(),:]\n",
    "            anchor_label = train_label[index.tolist()]\n",
    "            anchor_transfomr_dataset = MyDataset(anchor_data, anchor_label)\n",
    "            anchor_dl = DataLoader(anchor_transfomr_dataset, batch_size=args.batch_size,shuffle=True, num_workers=0,drop_last=False)\n",
    "            reconstructed_data, reconstructed_label, real_data = get_vae_simulated_data_from_sampling(model, anchor_dl)\n",
    "            reconstructed_data[reconstructed_data>torch.max(real_data)]=torch.max(real_data)\n",
    "            reconstructed_data[reconstructed_data<torch.min(real_data)]=torch.min(real_data)\n",
    "            reconstructed_data[torch.isnan(reconstructed_data)]=torch.max(real_data)\n",
    "\n",
    "            new_data = torch.cat((new_data,reconstructed_data),0)\n",
    "            new_label = torch.cat((new_label, reconstructed_label),0)\n",
    "            for i in range(aug_fold-1):\n",
    "                reconstructed_data, reconstructed_label,real_data = get_vae_simulated_data_from_sampling(model, anchor_dl)\n",
    "                reconstructed_data[reconstructed_data>torch.max(real_data)]=torch.max(real_data)\n",
    "                reconstructed_data[reconstructed_data<torch.min(real_data)]=torch.min(real_data)\n",
    "                reconstructed_data[torch.isnan(reconstructed_data)]=torch.max(real_data)\n",
    "                new_data = torch.cat((new_data,reconstructed_data),0)\n",
    "                new_label = torch.cat((new_label,reconstructed_label.to(device)),0)\n",
    "\n",
    "            reconstructed_data, reconstructed_label,real_data = get_vae_simulated_data_from_sampling(model, anchor_dl)\n",
    "            reconstructed_data[reconstructed_data>torch.max(real_data)]=torch.max(real_data)\n",
    "            reconstructed_data[reconstructed_data<torch.min(real_data)]=torch.min(real_data)\n",
    "            reconstructed_data[torch.isnan(reconstructed_data)]=torch.max(real_data)\n",
    "\n",
    "            #add remaining cell\n",
    "            N = range(np.array(train_minor[1])[j])\n",
    "            ds_index = random.sample(N, remaining_cell)\n",
    "            reconstructed_data = reconstructed_data[ds_index,:]\n",
    "            reconstructed_label = reconstructed_label[ds_index]\n",
    "            new_data = torch.cat((new_data,reconstructed_data),0)\n",
    "            new_label = torch.cat((new_label,reconstructed_label.to(device)),0)\n",
    "            j = j+1               \n",
    "\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.mkdir(model_save_path)\n",
    "        \n",
    "    #######load the model trained before augmentation#########\n",
    "    checkpoint_tar = os.path.join(model_save_path, 'model_best.pth.tar')\n",
    "    if os.path.exists(checkpoint_tar):\n",
    "        checkpoint = torch.load(checkpoint_tar)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "        print(\"load successfully\")\n",
    "\n",
    "    ############process new data after augmentation###########\n",
    "    train_transformed_dataset = MyDataset(new_data, new_label)\n",
    "    train_dl = DataLoader(train_transformed_dataset, batch_size=args.batch_size,shuffle=True, num_workers=0,drop_last=False)\n",
    "\n",
    "    ############## train model ###########\n",
    "    model,acc2,num1,train_num = train_model(model, train_dl, test_dl, lr=args.lr, epochs=int(args.epochs/2),classify_dim=classify_dim,best_top1_acc=0, save_path=model_save_path,feature_num=feature_num)\n",
    "    checkpoint_tar = os.path.join(model_save_path, 'model_best.pth.tar')\n",
    "    if os.path.exists(checkpoint_tar):\n",
    "        checkpoint = torch.load(checkpoint_tar)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "        print(\"load successfully\")\n",
    "    model,acc2,num1,train_num = train_model(model, train_dl, test_dl, lr=args.lr/10, epochs=int(args.epochs/2),classify_dim=classify_dim,best_top1_acc=0, save_path=model_save_path,feature_num=feature_num)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CD4', 'CD8a', 'CD366', 'CD279', 'CD117', 'Ly-6C', 'Ly-6G', 'CD19',\n",
       "       'CD45', 'CD25', 'CD11c', 'F4/80', 'I-A/I-E', 'NK-1.1', 'Ly-6A/E',\n",
       "       'CD274', 'CD86', 'CD192 (CCR2)', 'CD326', 'CD38', 'IgD', 'CD140a',\n",
       "       'CD11a', 'P2X7R', 'CD1d', 'Notch 4', 'CD31', 'Podoplanin', 'CD45R/B220',\n",
       "       'CD27', 'CD11b', 'CD202b'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\KARAN\\Desktop\\MultiOmics-Research\\STAGATE\\Landau\\SPOTS Landau paper dataset\\protein\\GSE198353_mmtv_pymt_ADT_t.csv',index_col=0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CD4   CD8a  CD366  CD279  CD117  Ly-6C  Ly-6G   CD19   CD45   CD25  \\\n",
      "0         0      1      2      3      4      5      6      7      8      9   \n",
      "1        32     33     34     35     36     37     38     39     40     41   \n",
      "2        64     65     66     67     68     69     70     71     72     73   \n",
      "3        96     97     98     99    100    101    102    103    104    105   \n",
      "4       128    129    130    131    132    133    134    135    136    137   \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "1973  63136  63137  63138  63139  63140  63141  63142  63143  63144  63145   \n",
      "1974  63168  63169  63170  63171  63172  63173  63174  63175  63176  63177   \n",
      "1975  63200  63201  63202  63203  63204  63205  63206  63207  63208  63209   \n",
      "1976  63232  63233  63234  63235  63236  63237  63238  63239  63240  63241   \n",
      "1977  63264  63265  63266  63267  63268  63269  63270  63271  63272  63273   \n",
      "\n",
      "      ...  CD11a  P2X7R   CD1d  Notch 4   CD31  Podoplanin  CD45R/B220   CD27  \\\n",
      "0     ...     22     23     24       25     26          27          28     29   \n",
      "1     ...     54     55     56       57     58          59          60     61   \n",
      "2     ...     86     87     88       89     90          91          92     93   \n",
      "3     ...    118    119    120      121    122         123         124    125   \n",
      "4     ...    150    151    152      153    154         155         156    157   \n",
      "...   ...    ...    ...    ...      ...    ...         ...         ...    ...   \n",
      "1973  ...  63158  63159  63160    63161  63162       63163       63164  63165   \n",
      "1974  ...  63190  63191  63192    63193  63194       63195       63196  63197   \n",
      "1975  ...  63222  63223  63224    63225  63226       63227       63228  63229   \n",
      "1976  ...  63254  63255  63256    63257  63258       63259       63260  63261   \n",
      "1977  ...  63286  63287  63288    63289  63290       63291       63292  63293   \n",
      "\n",
      "      CD11b  CD202b  \n",
      "0        30      31  \n",
      "1        62      63  \n",
      "2        94      95  \n",
      "3       126     127  \n",
      "4       158     159  \n",
      "...     ...     ...  \n",
      "1973  63166   63167  \n",
      "1974  63198   63199  \n",
      "1975  63230   63231  \n",
      "1976  63262   63263  \n",
      "1977  63294   63295  \n",
      "\n",
      "[1978 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\KARAN\\Desktop\\MultiOmics-Research\\STAGATE\\Landau\\SPOTS Landau paper dataset\\protein\\GSE198353_mmtv_pymt_ADT_t.csv',index_col=0)\n",
    "\n",
    "df = pd.DataFrame(np.arange(63296).reshape((1978,32)), columns=['CD4', 'CD8a', 'CD366', 'CD279', 'CD117', 'Ly-6C', 'Ly-6G', 'CD19',\n",
    "       'CD45', 'CD25', 'CD11c', 'F4/80', 'I-A/I-E', 'NK-1.1', 'Ly-6A/E',\n",
    "       'CD274', 'CD86', 'CD192 (CCR2)', 'CD326', 'CD38', 'IgD', 'CD140a',\n",
    "       'CD11a', 'P2X7R', 'CD1d', 'Notch 4', 'CD31', 'Podoplanin', 'CD45R/B220',\n",
    "       'CD27', 'CD11b', 'CD202b'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CD4   CD8a  CD366  CD279  CD117  Ly-6C  Ly-6G   CD19   CD45   CD25  \\\n",
      "0         0      1      2      3      4      5      6      7      8      9   \n",
      "1        32     33     34     35     36     37     38     39     40     41   \n",
      "2        64     65     66     67     68     69     70     71     72     73   \n",
      "3        96     97     98     99    100    101    102    103    104    105   \n",
      "4       128    129    130    131    132    133    134    135    136    137   \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "1973  63136  63137  63138  63139  63140  63141  63142  63143  63144  63145   \n",
      "1974  63168  63169  63170  63171  63172  63173  63174  63175  63176  63177   \n",
      "1975  63200  63201  63202  63203  63204  63205  63206  63207  63208  63209   \n",
      "1976  63232  63233  63234  63235  63236  63237  63238  63239  63240  63241   \n",
      "1977  63264  63265  63266  63267  63268  63269  63270  63271  63272  63273   \n",
      "\n",
      "      ...  CD11a  P2X7R   CD1d  Notch 4   CD31  Podoplanin  CD45R/B220   CD27  \\\n",
      "0     ...     22     23     24       25     26          27          28     29   \n",
      "1     ...     54     55     56       57     58          59          60     61   \n",
      "2     ...     86     87     88       89     90          91          92     93   \n",
      "3     ...    118    119    120      121    122         123         124    125   \n",
      "4     ...    150    151    152      153    154         155         156    157   \n",
      "...   ...    ...    ...    ...      ...    ...         ...         ...    ...   \n",
      "1973  ...  63158  63159  63160    63161  63162       63163       63164  63165   \n",
      "1974  ...  63190  63191  63192    63193  63194       63195       63196  63197   \n",
      "1975  ...  63222  63223  63224    63225  63226       63227       63228  63229   \n",
      "1976  ...  63254  63255  63256    63257  63258       63259       63260  63261   \n",
      "1977  ...  63286  63287  63288    63289  63290       63291       63292  63293   \n",
      "\n",
      "      CD11b  CD202b  \n",
      "0        30      31  \n",
      "1        62      63  \n",
      "2        94      95  \n",
      "3       126     127  \n",
      "4       158     159  \n",
      "...     ...     ...  \n",
      "1973  63166   63167  \n",
      "1974  63198   63199  \n",
      "1975  63230   63231  \n",
      "1976  63262   63263  \n",
      "1977  63294   63295  \n",
      "\n",
      "[1978 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save to HDF5\n",
    "filename = 'adt.h5'\n",
    "\n",
    "df.to_hdf(filename, 'matrix/data', mode='w', format='table')\n",
    "del df    # allow df to be garbage collected\n",
    "\n",
    "print(pd.read_hdf(filename, 'matrix/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\anndata\\_core\\anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "adata=sc.read_10x_h5(r'C:\\Users\\KARAN\\Desktop\\Matilda\\data\\landau\\protein\\GSE198353_mmtv_pymt_GEX_filtered_feature_bc_matrix.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_ids</th>\n",
       "      <th>feature_types</th>\n",
       "      <th>genome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Xkr4</th>\n",
       "      <td>ENSMUSG00000051951</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gm1992</th>\n",
       "      <td>ENSMUSG00000089699</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gm19938</th>\n",
       "      <td>ENSMUSG00000102331</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gm37381</th>\n",
       "      <td>ENSMUSG00000102343</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rp1</th>\n",
       "      <td>ENSMUSG00000025900</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC133095.2</th>\n",
       "      <td>ENSMUSG00000095475</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC133095.1</th>\n",
       "      <td>ENSMUSG00000094855</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC234645.1</th>\n",
       "      <td>ENSMUSG00000095019</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC149090.1</th>\n",
       "      <td>ENSMUSG00000095041</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PyMT</th>\n",
       "      <td>PyMT</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>MMTV_PyMT_gex-mm10-2020-A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32286 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      gene_ids    feature_types                     genome\n",
       "Xkr4        ENSMUSG00000051951  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "Gm1992      ENSMUSG00000089699  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "Gm19938     ENSMUSG00000102331  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "Gm37381     ENSMUSG00000102343  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "Rp1         ENSMUSG00000025900  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "...                        ...              ...                        ...\n",
       "AC133095.2  ENSMUSG00000095475  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "AC133095.1  ENSMUSG00000094855  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "AC234645.1  ENSMUSG00000095019  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "AC149090.1  ENSMUSG00000095041  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "PyMT                      PyMT  Gene Expression  MMTV_PyMT_gex-mm10-2020-A\n",
       "\n",
       "[32286 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63861708"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1978 * 32286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"data\": shape (7378559,), type \"<i4\">"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = h5py.File(r'C:\\Users\\KARAN\\Desktop\\Matilda\\data\\landau\\protein\\GSE198353_mmtv_pymt_GEX_filtered_feature_bc_matrix.h5',\"r\")\n",
    "h5_data = data['matrix/data']\n",
    "h5_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_data = scipy.sparse.csr_matrix(np.array(h5_data).transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7378559 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 7378559 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fs = torch.from_numpy(np.array(sparse_data.todense()))\n",
    "data_fs = Variable(data_fs.type(FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7378559])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\KARAN\\Desktop\\Matilda\\data\\landau\\protein\\GSE198353_mmtv_pymt_ADT_t.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['AAACAAGTATCTCCCA-1', 478, 583, ..., 1067, 1193, 587],\n",
       "       ['AAACACCAATAACTGC-1', 1504, 1217, ..., 2040, 3056, 1000],\n",
       "       ['AAACAGGGTCTATATT-1', 1526, 1231, ..., 2193, 3863, 985],\n",
       "       ...,\n",
       "       ['TTGTTTCATTAGTCTA-1', 661, 528, ..., 948, 801, 404],\n",
       "       ['TTGTTTCCATACAACT-1', 1031, 857, ..., 1582, 3864, 652],\n",
       "       ['TTGTTTGTATTACACG-1', 861, 720, ..., 1124, 1002, 492]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIELD1</th>\n",
       "      <th>CD4</th>\n",
       "      <th>CD8a</th>\n",
       "      <th>CD366</th>\n",
       "      <th>CD279</th>\n",
       "      <th>CD117</th>\n",
       "      <th>Ly-6C</th>\n",
       "      <th>Ly-6G</th>\n",
       "      <th>CD19</th>\n",
       "      <th>CD45</th>\n",
       "      <th>...</th>\n",
       "      <th>CD11a</th>\n",
       "      <th>P2X7R</th>\n",
       "      <th>CD1d</th>\n",
       "      <th>Notch 4</th>\n",
       "      <th>CD31</th>\n",
       "      <th>Podoplanin</th>\n",
       "      <th>CD45R/B220</th>\n",
       "      <th>CD27</th>\n",
       "      <th>CD11b</th>\n",
       "      <th>CD202b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAACAAGTATCTCCCA-1</td>\n",
       "      <td>478</td>\n",
       "      <td>583</td>\n",
       "      <td>877</td>\n",
       "      <td>446</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>481</td>\n",
       "      <td>434</td>\n",
       "      <td>3157</td>\n",
       "      <td>...</td>\n",
       "      <td>638</td>\n",
       "      <td>1222</td>\n",
       "      <td>1253</td>\n",
       "      <td>1273</td>\n",
       "      <td>1354</td>\n",
       "      <td>4858</td>\n",
       "      <td>656</td>\n",
       "      <td>1067</td>\n",
       "      <td>1193</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAACACCAATAACTGC-1</td>\n",
       "      <td>1504</td>\n",
       "      <td>1217</td>\n",
       "      <td>1731</td>\n",
       "      <td>943</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1027</td>\n",
       "      <td>933</td>\n",
       "      <td>6580</td>\n",
       "      <td>...</td>\n",
       "      <td>1808</td>\n",
       "      <td>2240</td>\n",
       "      <td>1932</td>\n",
       "      <td>2253</td>\n",
       "      <td>3095</td>\n",
       "      <td>10214</td>\n",
       "      <td>1266</td>\n",
       "      <td>2040</td>\n",
       "      <td>3056</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAACAGGGTCTATATT-1</td>\n",
       "      <td>1526</td>\n",
       "      <td>1231</td>\n",
       "      <td>1433</td>\n",
       "      <td>849</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1322</td>\n",
       "      <td>1515</td>\n",
       "      <td>5964</td>\n",
       "      <td>...</td>\n",
       "      <td>1778</td>\n",
       "      <td>2120</td>\n",
       "      <td>1971</td>\n",
       "      <td>2216</td>\n",
       "      <td>2927</td>\n",
       "      <td>2700</td>\n",
       "      <td>1437</td>\n",
       "      <td>2193</td>\n",
       "      <td>3863</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAACAGTGTTCCTGGG-1</td>\n",
       "      <td>847</td>\n",
       "      <td>787</td>\n",
       "      <td>1028</td>\n",
       "      <td>517</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>610</td>\n",
       "      <td>567</td>\n",
       "      <td>3476</td>\n",
       "      <td>...</td>\n",
       "      <td>939</td>\n",
       "      <td>1266</td>\n",
       "      <td>1242</td>\n",
       "      <td>1268</td>\n",
       "      <td>1742</td>\n",
       "      <td>4985</td>\n",
       "      <td>881</td>\n",
       "      <td>1230</td>\n",
       "      <td>1046</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAACATGGTGAGAGGA-1</td>\n",
       "      <td>2317</td>\n",
       "      <td>1770</td>\n",
       "      <td>2347</td>\n",
       "      <td>1475</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>1802</td>\n",
       "      <td>2371</td>\n",
       "      <td>7370</td>\n",
       "      <td>...</td>\n",
       "      <td>2573</td>\n",
       "      <td>3359</td>\n",
       "      <td>2859</td>\n",
       "      <td>3335</td>\n",
       "      <td>4107</td>\n",
       "      <td>6983</td>\n",
       "      <td>2777</td>\n",
       "      <td>3766</td>\n",
       "      <td>4316</td>\n",
       "      <td>1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>TTGTTGGCAATGACTG-1</td>\n",
       "      <td>1669</td>\n",
       "      <td>1378</td>\n",
       "      <td>1766</td>\n",
       "      <td>932</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1070</td>\n",
       "      <td>957</td>\n",
       "      <td>6174</td>\n",
       "      <td>...</td>\n",
       "      <td>1595</td>\n",
       "      <td>2025</td>\n",
       "      <td>1963</td>\n",
       "      <td>2319</td>\n",
       "      <td>2821</td>\n",
       "      <td>6929</td>\n",
       "      <td>1472</td>\n",
       "      <td>2130</td>\n",
       "      <td>1904</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>TTGTTTCACATCCAGG-1</td>\n",
       "      <td>903</td>\n",
       "      <td>693</td>\n",
       "      <td>919</td>\n",
       "      <td>523</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>613</td>\n",
       "      <td>623</td>\n",
       "      <td>3531</td>\n",
       "      <td>...</td>\n",
       "      <td>917</td>\n",
       "      <td>1395</td>\n",
       "      <td>1224</td>\n",
       "      <td>1319</td>\n",
       "      <td>1623</td>\n",
       "      <td>9180</td>\n",
       "      <td>870</td>\n",
       "      <td>1226</td>\n",
       "      <td>1119</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>TTGTTTCATTAGTCTA-1</td>\n",
       "      <td>661</td>\n",
       "      <td>528</td>\n",
       "      <td>729</td>\n",
       "      <td>477</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>404</td>\n",
       "      <td>2571</td>\n",
       "      <td>...</td>\n",
       "      <td>714</td>\n",
       "      <td>984</td>\n",
       "      <td>952</td>\n",
       "      <td>1057</td>\n",
       "      <td>1242</td>\n",
       "      <td>4286</td>\n",
       "      <td>699</td>\n",
       "      <td>948</td>\n",
       "      <td>801</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>TTGTTTCCATACAACT-1</td>\n",
       "      <td>1031</td>\n",
       "      <td>857</td>\n",
       "      <td>1250</td>\n",
       "      <td>666</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>951</td>\n",
       "      <td>938</td>\n",
       "      <td>5599</td>\n",
       "      <td>...</td>\n",
       "      <td>1425</td>\n",
       "      <td>1454</td>\n",
       "      <td>1123</td>\n",
       "      <td>1547</td>\n",
       "      <td>2710</td>\n",
       "      <td>2784</td>\n",
       "      <td>1106</td>\n",
       "      <td>1582</td>\n",
       "      <td>3864</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>TTGTTTGTATTACACG-1</td>\n",
       "      <td>861</td>\n",
       "      <td>720</td>\n",
       "      <td>1080</td>\n",
       "      <td>525</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>527</td>\n",
       "      <td>560</td>\n",
       "      <td>3401</td>\n",
       "      <td>...</td>\n",
       "      <td>972</td>\n",
       "      <td>1273</td>\n",
       "      <td>1005</td>\n",
       "      <td>1221</td>\n",
       "      <td>1582</td>\n",
       "      <td>6642</td>\n",
       "      <td>795</td>\n",
       "      <td>1124</td>\n",
       "      <td>1002</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1978 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  FIELD1   CD4  CD8a  CD366  CD279  CD117  Ly-6C  Ly-6G  CD19  \\\n",
       "0     AAACAAGTATCTCCCA-1   478   583    877    446     57      0    481   434   \n",
       "1     AAACACCAATAACTGC-1  1504  1217   1731    943     64      2   1027   933   \n",
       "2     AAACAGGGTCTATATT-1  1526  1231   1433    849     23      1   1322  1515   \n",
       "3     AAACAGTGTTCCTGGG-1   847   787   1028    517     67      0    610   567   \n",
       "4     AAACATGGTGAGAGGA-1  2317  1770   2347   1475     58      1   1802  2371   \n",
       "...                  ...   ...   ...    ...    ...    ...    ...    ...   ...   \n",
       "1973  TTGTTGGCAATGACTG-1  1669  1378   1766    932     60      0   1070   957   \n",
       "1974  TTGTTTCACATCCAGG-1   903   693    919    523     61      2    613   623   \n",
       "1975  TTGTTTCATTAGTCTA-1   661   528    729    477     23      1    512   404   \n",
       "1976  TTGTTTCCATACAACT-1  1031   857   1250    666     40      0    951   938   \n",
       "1977  TTGTTTGTATTACACG-1   861   720   1080    525     44      3    527   560   \n",
       "\n",
       "      CD45  ...  CD11a  P2X7R  CD1d  Notch 4  CD31  Podoplanin  CD45R/B220  \\\n",
       "0     3157  ...    638   1222  1253     1273  1354        4858         656   \n",
       "1     6580  ...   1808   2240  1932     2253  3095       10214        1266   \n",
       "2     5964  ...   1778   2120  1971     2216  2927        2700        1437   \n",
       "3     3476  ...    939   1266  1242     1268  1742        4985         881   \n",
       "4     7370  ...   2573   3359  2859     3335  4107        6983        2777   \n",
       "...    ...  ...    ...    ...   ...      ...   ...         ...         ...   \n",
       "1973  6174  ...   1595   2025  1963     2319  2821        6929        1472   \n",
       "1974  3531  ...    917   1395  1224     1319  1623        9180         870   \n",
       "1975  2571  ...    714    984   952     1057  1242        4286         699   \n",
       "1976  5599  ...   1425   1454  1123     1547  2710        2784        1106   \n",
       "1977  3401  ...    972   1273  1005     1221  1582        6642         795   \n",
       "\n",
       "      CD27  CD11b  CD202b  \n",
       "0     1067   1193     587  \n",
       "1     2040   3056    1000  \n",
       "2     2193   3863     985  \n",
       "3     1230   1046     634  \n",
       "4     3766   4316    1723  \n",
       "...    ...    ...     ...  \n",
       "1973  2130   1904    1053  \n",
       "1974  1226   1119     569  \n",
       "1975   948    801     404  \n",
       "1976  1582   3864     652  \n",
       "1977  1124   1002     492  \n",
       "\n",
       "[1978 rows x 33 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def read_csv_data(data_path):\n",
    "    # Read CSV file using pandas\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Extract the data column from the CSV dataframe\n",
    "    csv_data = df['data']\n",
    "\n",
    "    # Convert the data column to a sparse CSR matrix\n",
    "    sparse_data = scipy.sparse.csr_matrix(csv_data.values.reshape(-1, 1))\n",
    "\n",
    "    # Convert the sparse matrix to a dense numpy array\n",
    "    dense_data = sparse_data.toarray()\n",
    "\n",
    "    # Convert the dense array to a PyTorch tensor\n",
    "    data_fs = torch.from_numpy(dense_data)\n",
    "\n",
    "    return data_fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m b \u001b[39m=\u001b[39m read_csv_data(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mKARAN\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mMatilda\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mlandau\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mprotein\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGSE198353_mmtv_pymt_ADT_t.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[52], line 11\u001b[0m, in \u001b[0;36mread_csv_data\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(data_path)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Extract the data column from the CSV dataframe\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m csv_data \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     13\u001b[0m \u001b[39m# Convert the data column to a sparse CSR matrix\u001b[39;00m\n\u001b[0;32m     14\u001b[0m sparse_data \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mcsr_matrix(csv_data\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "b = read_csv_data(r'C:\\Users\\KARAN\\Desktop\\Matilda\\data\\landau\\protein\\GSE198353_mmtv_pymt_ADT_t.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7378559])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_train.py:2: DeprecationWarning: The parser module is deprecated and will be removed in future versions of Python\n",
      "  import parser\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_train.py\", line 54, in <module>\n",
      "    train_adt_data = read_h5_data(train_adt_data_path)\n",
      "  File \"c:\\Users\\KARAN\\Desktop\\Matilda\\main\\util.py\", line 130, in read_h5_data\n",
      "    data = h5py.File(data_path,\"r\")\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\h5py\\_hl\\files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\h5py\\_hl\\files.py\", line 231, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (file signature not found)\n"
     ]
    }
   ],
   "source": [
    "!python main_matilda_train.py --rna ../data/landau/protein/GSE198353_mmtv_pymt_GEX_filtered_feature_bc_matrix.h5 --adt ../data/landau/protein/GSE198353_mmtv_pymt_ADT_t.csv  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_train.py:2: DeprecationWarning: The parser module is deprecated and will be removed in future versions of Python\n",
      "  import parser\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_train.py\", line 55, in <module>\n",
      "    train_label = read_fs_label(train_label_path)\n",
      "  File \"c:\\Users\\KARAN\\Desktop\\Matilda\\main\\util.py\", line 141, in read_fs_label\n",
      "    label_fs = pd.read_csv(label_path,header=None,index_col=False)  #\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 950, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 605, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1442, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1735, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\common.py\", line 856, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'NULL'\n"
     ]
    }
   ],
   "source": [
    "!python main_matilda_train.py --rna ../data/TEAseq/train_rna.h5 --adt ../data/TEAseq/train_adt.h5  --cty ../data/TEAseq/train_cty.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is CITEseq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_task.py:2: DeprecationWarning: The parser module is deprecated and will be removed in future versions of Python\n",
      "  import parser\n",
      "c:\\Users\\KARAN\\Desktop\\Matilda\\main\\learn\\predict.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a = torch.max(nn.Softmax()(x_cty),1)\n"
     ]
    }
   ],
   "source": [
    "!python main_matilda_task.py --rna ../data/TEAseq/test_rna.h5 --adt ../data/TEAseq/test_adt.h5 --cty ../data/TEAseq/test_cty.csv --classification True --query True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is CITEseq\n",
      "simulate celltype index: 1 \t cell type name: B.Naive\n",
      "finish simulation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_task.py:2: DeprecationWarning: The parser module is deprecated and will be removed in future versions of Python\n",
      "  import parser\n"
     ]
    }
   ],
   "source": [
    "!python main_matilda_task.py --rna ../data/TEAseq/train_rna.h5 --adt ../data/TEAseq/train_adt.h5 --cty ../data/TEAseq/train_cty.csv --simulation True --simulation_ct 1 --simulation_num 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is CITEseq\n",
      "finish dimension reduction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_task.py:2: DeprecationWarning: The parser module is deprecated and will be removed in future versions of Python\n",
      "  import parser\n"
     ]
    }
   ],
   "source": [
    "!python main_matilda_task.py --rna ../data/TEAseq/train_rna.h5 --adt ../data/TEAseq/train_adt.h5 --cty ../data/TEAseq/train_cty.csv --dim_reduce True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_task.py:2: DeprecationWarning: The parser module is deprecated and will be removed in future versions of Python\n",
      "  import parser\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\KARAN\\Desktop\\Matilda\\main\\main_matilda_task.py\", line 68, in <module>\n",
      "    label = read_fs_label(label_path)\n",
      "  File \"c:\\Users\\KARAN\\Desktop\\Matilda\\main\\util.py\", line 141, in read_fs_label\n",
      "    label_fs = pd.read_csv(label_path,header=None,index_col=False)  #\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 950, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 605, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1442, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1735, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "  File \"c:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\pandas\\io\\common.py\", line 856, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'NULL'\n"
     ]
    }
   ],
   "source": [
    "!python main_matilda_task.py --rna ../data/TEAseq/train_rna.h5 --adt ../data/TEAseq/train_adt.h5   --fs True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
