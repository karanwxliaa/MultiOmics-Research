{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ''\n",
    "temp = sc.read_visium(file, count_file=r'GSE198353_mmtv_pymt_GEX_filtered_feature_bc_matrix.h5',load_images=True)\n",
    "temp.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = sc.read_csv('GSE198353_mmtv_pymt_ADT_t.csv')\n",
    "pdata.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata.obsm=temp.obsm\n",
    "pdata.uns = temp.uns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1978 × 32\n",
       "    uns: 'spatial'\n",
       "    obsm: 'spatial'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ''\n",
    "gdata = sc.read_visium(file, count_file=r'GSE198353_mmtv_pymt_GEX_filtered_feature_bc_matrix.h5',load_images=True)\n",
    "gdata.var_names_make_unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCH 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arch1 (Multi-Omic Autoencoder with Spatial Attention):\n",
    "Arch1 is a multi-omic integration model that incorporates gene expression, spatial information, and protein data using an autoencoder architecture with a spatial attention mechanism. The input data consists of an Anndata gene expression matrix, spatial information obtained using scanpy.read_visium, and a protein data matrix with counts of different proteins.\n",
    "Arch1 utilizes a variational autoencoder (VAE) as the core component for learning a common latent representation of the multi-omic data. It consists of a single encoder that takes the combined gene expression and protein data as input and maps it to a latent space. The VAE incorporates both the reconstruction loss and the Kullback-Leibler (KL) divergence loss to optimize the latent representation.\n",
    "\n",
    "In addition to the VAE, Arch1 incorporates a spatial attention mechanism to capture relevant spatial features. It uses the spatial coordinates (obtained from adata.obsm and adata.uns) to highlight the spatial information within the latent space representation. The spatial attention module is designed to improve the accuracy of the latent representation by reducing the loss from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Define the Variational Autoencoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(256, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, 256)\n",
    "        self.fc4 = nn.Linear(256, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(x)\n",
    "        logvar = self.fc2_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.relu(self.fc3(z))\n",
    "        x = self.fc4(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mean, logvar\n",
    "\n",
    "# Define the Adaptive Graph Attention Layer\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(out_features, 2*out_features)))\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = self.fc(input)\n",
    "        e = torch.matmul(h, self.a)\n",
    "        attention = F.softmax(e, dim=1)  # Compute softmax along dimension 1\n",
    "        attention = torch.transpose(attention, 0, 1)  # Transpose attention tensor\n",
    "        output = torch.spmm(adj, attention.t())  # Transpose attention tensor again\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Define the \"arch\" module\n",
    "class Arch(nn.Module):\n",
    "    def __init__(self, input_dim, spatial_input_dim, latent_dim):\n",
    "        super(Arch, self).__init__()\n",
    "\n",
    "        # Autoencoder\n",
    "        self.autoencoder = VAE(input_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "\n",
    "        # Spatial Attention\n",
    "        self.spatial_attention = GraphAttentionLayer(2, latent_dim)\n",
    "\n",
    "    def forward(self, data, spatial_data, adj):\n",
    "        # Autoencoder\n",
    "        recon, mean, logvar = self.autoencoder(data)\n",
    "        \n",
    "        # Spatial Attention\n",
    "        spatial_attention_weights = self.spatial_attention(spatial_data, adj)\n",
    "\n",
    "        # Combined Latent Representation\n",
    "        combined_latent = mean + spatial_attention_weights\n",
    "\n",
    "        # Decoder\n",
    "        recon = self.decoder(combined_latent)\n",
    "\n",
    "        return recon, mean, logvar\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc of stagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "\n",
    "# Extract spatial information\n",
    "spatial_data = gdata.obsm['spatial']\n",
    "\n",
    "# Set the spatial distance threshold\n",
    "distance_threshold = 10.0\n",
    "\n",
    "# Compute pairwise distances between spatial points\n",
    "distances = cdist(spatial_data, spatial_data)\n",
    "\n",
    "# Create adjacency matrix based on distance threshold\n",
    "adjacency = np.where(distances <= distance_threshold, 1, 0)\n",
    "\n",
    "# Convert adjacency matrix to a sparse matrix format\n",
    "adj_sparse = sp.coo_matrix(adjacency)\n",
    "\n",
    "# Convert adjacency matrix to a dense tensor\n",
    "adj_tensor = torch.tensor(adj_sparse.toarray()).float()\n",
    "adj = adj_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1978x1978 and 32x200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     42\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m recon, mean, logvar \u001b[39m=\u001b[39m model(batch[\u001b[39m0\u001b[39;49m], batch[\u001b[39m1\u001b[39;49m], adj)  \u001b[39m# Pass the adj argument\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m# Compute the reconstruction loss\u001b[39;00m\n\u001b[0;32m     46\u001b[0m recon_loss \u001b[39m=\u001b[39m reconstruction_loss(recon, batch[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[67], line 82\u001b[0m, in \u001b[0;36mArch.forward\u001b[1;34m(self, data, spatial_data, adj)\u001b[0m\n\u001b[0;32m     79\u001b[0m recon, mean, logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautoencoder(data)\n\u001b[0;32m     81\u001b[0m \u001b[39m# Spatial Attention\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m spatial_attention_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspatial_attention(spatial_data, adj)\n\u001b[0;32m     84\u001b[0m \u001b[39m# Combined Latent Representation\u001b[39;00m\n\u001b[0;32m     85\u001b[0m combined_latent \u001b[39m=\u001b[39m mean \u001b[39m+\u001b[39m spatial_attention_weights\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[67], line 58\u001b[0m, in \u001b[0;36mGraphAttentionLayer.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     56\u001b[0m attention \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(e, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Compute softmax along dimension 1\u001b[39;00m\n\u001b[0;32m     57\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtranspose(attention, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# Transpose attention tensor\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mspmm(adj, attention\u001b[39m.\u001b[39;49mt())  \u001b[39m# Transpose attention tensor again\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1978x1978 and 32x200)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load gene expression matrix using Anndata\n",
    "data = torch.tensor(gdata.X.toarray()).float()\n",
    "\n",
    "# Load spatial information from gdata.obsm['spatial']\n",
    "spatial_data = torch.tensor(gdata.obsm['spatial']).float()\n",
    "\n",
    "\n",
    "# Get the input dimensions\n",
    "input_dim = data.shape[1]\n",
    "spatial_input_dim = spatial_data.shape[1]\n",
    "\n",
    "# Set the latent dimension\n",
    "latent_dim = 100\n",
    "\n",
    "# Create an instance of the \"arch\" module\n",
    "model = Arch(input_dim, spatial_input_dim, latent_dim)\n",
    "\n",
    "# Define the reconstruction loss\n",
    "reconstruction_loss = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "data = torch.tensor(data).float()\n",
    "spatial_data = torch.tensor(spatial_data).float()\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "dataset = torch.utils.data.TensorDataset(data, spatial_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        recon, mean, logvar = model(batch[0], batch[1], adj)  # Pass the adj argument\n",
    "\n",
    "        # Compute the reconstruction loss\n",
    "        recon_loss = reconstruction_loss(recon, batch[0])\n",
    "\n",
    "        # Compute the KL divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "        # Compute the total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCH 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arch2 (Multi-Omic Autoencoder with Separate Encoders and Spatial Attention):\n",
    "Arch2 is a multi-omic integration model that learns gene expression and protein data separately using two separate encoders and maps them to a shared latent space. It also incorporates spatial information and utilizes a spatial attention mechanism to improve the latent space representations.\n",
    "Arch2 consists of two encoders, one for gene expression data and another for protein data. Each encoder independently processes its respective input and maps it to a separate latent space. The encoders utilize a suitable architecture (e.g., variational autoencoder) to capture the specific characteristics of each omic dataset.\n",
    "\n",
    "After obtaining the separate latent representations, Arch2 incorporates an adaptive graph attention layer to incorporate spatial information within the latent space. This attention layer dynamically assigns importance weights to spatial features based on their relevance. The spatial attention mechanism enables the model to focus on spatially informative regions and enhance the overall integration of multi-omic data.\n",
    "\n",
    "Finally, Arch2 utilizes a shared decoder that takes the combined latent representation as input and reconstructs the original multi-omic data. The decoder aims to minimize the loss between the reconstructed data and the original input, improving the accuracy of the learned latent representations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import scanpy as sc\n",
    "\n",
    "\n",
    "# Define the Variational Autoencoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2_mean = nn.Linear(256, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(256, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, 256)\n",
    "        self.fc4 = nn.Linear(256, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(x)\n",
    "        logvar = self.fc2_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.relu(self.fc3(z))\n",
    "        x = self.fc4(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mean, logvar\n",
    "\n",
    "# Define the Adaptive Graph Attention Layer\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.a = nn.Parameter(torch.zeros(out_features, 1))\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = self.fc(input)\n",
    "        e = self.leakyrelu(torch.matmul(h, self.a))\n",
    "        attention = torch.sparse_softmax(torch.transpose(adj, 0, 1))\n",
    "        output = torch.spmm(attention, h)\n",
    "        return output\n",
    "\n",
    "# Define the \"arch2\" module\n",
    "class Arch2(nn.Module):\n",
    "    def __init__(self, gene_input_dim, protein_input_dim, spatial_input_dim, latent_dim):\n",
    "        super(Arch2, self).__init__()\n",
    "\n",
    "        # Gene Encoder\n",
    "        self.gene_encoder = VAE(gene_input_dim, latent_dim)\n",
    "\n",
    "        # Protein Encoder\n",
    "        self.protein_encoder = VAE(protein_input_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(latent_dim, gene_input_dim)\n",
    "\n",
    "        # Spatial Attention\n",
    "        self.spatial_attention = GraphAttentionLayer(2, latent_dim)\n",
    "\n",
    "    def forward(self, gene_data, protein_data, spatial_data):\n",
    "        # Gene Encoder\n",
    "        gene_recon, gene_mean, gene_logvar = self.gene_encoder(gene_data)\n",
    "\n",
    "        # Protein Encoder\n",
    "        protein_recon, protein_mean, protein_logvar = self.protein_encoder(protein_data)\n",
    "\n",
    "        # Spatial Attention\n",
    "        spatial_attention_weights = self.spatial_attention(spatial_data, adj)\n",
    "\n",
    "        # Combined Latent Representation\n",
    "        combined_latent = gene_mean + protein_mean + spatial_attention_weights\n",
    "\n",
    "        # Decoder\n",
    "        gene_recon = self.decoder(combined_latent)\n",
    "\n",
    "        return gene_recon, gene_mean, gene_logvar\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc of STAGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinEncoder(nn.Module):\n",
    "    def __init__(self, protein_dim, latent_dim):\n",
    "        super(ProteinEncoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(protein_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeneEncoder(nn.Module):\n",
    "    def __init__(self, gene_dim, latent_dim):\n",
    "        super(GeneEncoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(gene_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m protein_data \u001b[39m=\u001b[39m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Load spatial information using scanpy.read_visium\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m spatial_data \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mread_visium(\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Get the input dimensions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m gene_input_dim \u001b[39m=\u001b[39m gene_data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\site-packages\\scanpy\\readwrite.py:359\u001b[0m, in \u001b[0;36mread_visium\u001b[1;34m(path, genome, count_file, library_id, load_images, source_image_path)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_visium\u001b[39m(\n\u001b[0;32m    298\u001b[0m     path: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m    299\u001b[0m     genome: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     source_image_path: Optional[Union[\u001b[39mstr\u001b[39m, Path]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    305\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AnnData:\n\u001b[0;32m    306\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\\\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m    Read 10x-Genomics-formatted visum dataset.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39m        Spatial spot coordinates, usable as `basis` by :func:`~scanpy.pl.embedding`.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     path \u001b[39m=\u001b[39m Path(path)\n\u001b[0;32m    360\u001b[0m     adata \u001b[39m=\u001b[39m read_10x_h5(path \u001b[39m/\u001b[39m count_file, genome\u001b[39m=\u001b[39mgenome)\n\u001b[0;32m    362\u001b[0m     adata\u001b[39m.\u001b[39muns[\u001b[39m\"\u001b[39m\u001b[39mspatial\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\pathlib.py:1082\u001b[0m, in \u001b[0;36mPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m Path:\n\u001b[0;32m   1081\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m WindowsPath \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnt\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m PosixPath\n\u001b[1;32m-> 1082\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_parts(args, init\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m   1083\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flavour\u001b[39m.\u001b[39mis_supported:\n\u001b[0;32m   1084\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot instantiate \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m on your system\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m                               \u001b[39m%\u001b[39m (\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,))\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\pathlib.py:707\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[1;34m(cls, args, init)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_from_parts\u001b[39m(\u001b[39mcls\u001b[39m, args, init\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    704\u001b[0m     \u001b[39m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[39m# right flavour.\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[1;32m--> 707\u001b[0m     drv, root, parts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_args(args)\n\u001b[0;32m    708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drv \u001b[39m=\u001b[39m drv\n\u001b[0;32m    709\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_root \u001b[39m=\u001b[39m root\n",
      "File \u001b[1;32mc:\\Users\\KARAN\\anaconda3\\envs\\STAGATE_PT\\lib\\pathlib.py:691\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[1;34m(cls, args)\u001b[0m\n\u001b[0;32m    689\u001b[0m     parts \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m a\u001b[39m.\u001b[39m_parts\n\u001b[0;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 691\u001b[0m     a \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mfspath(a)\n\u001b[0;32m    692\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    693\u001b[0m         \u001b[39m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[0;32m    694\u001b[0m         parts\u001b[39m.\u001b[39mappend(\u001b[39mstr\u001b[39m(a))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not ellipsis"
     ]
    }
   ],
   "source": [
    "# Load gene expression matrix using Anndata\n",
    "gene_data = ...\n",
    "\n",
    "# Load protein data matrix\n",
    "protein_data = ...\n",
    "\n",
    "# Load spatial information using scanpy.read_visium\n",
    "spatial_data = sc.read_visium(...)\n",
    "\n",
    "# Get the input dimensions\n",
    "gene_input_dim = gene_data.shape[1]\n",
    "protein_input_dim = protein_data.shape[1]\n",
    "spatial_input_dim = spatial_data.shape[1]\n",
    "\n",
    "# Set the latent dimension\n",
    "latent_dim = 100\n",
    "\n",
    "# Create an instance of the \"arch2\" module\n",
    "model = Arch2(gene_input_dim, protein_input_dim, spatial_input_dim, latent_dim)\n",
    "\n",
    "# Define the loss function\n",
    "reconstruction_loss = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "gene_data = torch.tensor(gene_data).float()\n",
    "protein_data = torch.tensor(protein_data).float()\n",
    "spatial_data = torch.tensor(spatial_data).float()\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "dataset = torch.utils.data.TensorDataset(gene_data, protein_data, spatial_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        gene_recon, gene_mean, gene_logvar = model(batch[0], batch[1], batch[2])\n",
    "\n",
    "        # Compute the reconstruction loss\n",
    "        recon_loss = reconstruction_loss(gene_recon, batch[0])\n",
    "\n",
    "        # Compute the KL divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + gene_logvar - gene_mean.pow(2) - gene_logvar.exp())\n",
    "\n",
    "        # Compute the total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAGATE_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
